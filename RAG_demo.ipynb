{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "acef41c6d27e4b5f95fb9a1f0509f0aa": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "VBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [],
            "layout": "IPY_MODEL_a3d32ab337aa4e9eb5cb57037d9f32c3"
          }
        },
        "c83a5c940b2f4c1a9250c30d980fc1da": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_966b94f9bf1a4c229e071aa6d6dd749d",
            "placeholder": "​",
            "style": "IPY_MODEL_b4ea870eb86c4c6bb19908f148f6b21c",
            "value": "<center> <img\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svg\nalt='Hugging Face'> <br> Copy a token from <a\nhref=\"https://huggingface.co/settings/tokens\" target=\"_blank\">your Hugging Face\ntokens page</a> and paste it below. <br> Immediately click login after copying\nyour token or it might be stored in plain text in this notebook file. </center>"
          }
        },
        "c2547ac8c5d64d73b3f34a2dd67cc3ed": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "PasswordModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "PasswordModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "PasswordView",
            "continuous_update": true,
            "description": "Token:",
            "description_tooltip": null,
            "disabled": false,
            "layout": "IPY_MODEL_ac14c239ddd74f089cfb3fa09795523c",
            "placeholder": "​",
            "style": "IPY_MODEL_aad28023b105476aa9316ed90b1efe3b",
            "value": ""
          }
        },
        "683170a03b4f45c9918da174b92f005f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "CheckboxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "CheckboxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "CheckboxView",
            "description": "Add token as git credential?",
            "description_tooltip": null,
            "disabled": false,
            "indent": true,
            "layout": "IPY_MODEL_eb91c481ba814dd7acf0c1ac2754ac25",
            "style": "IPY_MODEL_1bc3c8d3483543e190a59f1605d56a20",
            "value": true
          }
        },
        "e2a854200d7042298702b0141271256d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ButtonView",
            "button_style": "",
            "description": "Login",
            "disabled": false,
            "icon": "",
            "layout": "IPY_MODEL_4164ada8f2694f66b66a3e02d577842b",
            "style": "IPY_MODEL_ad38f5d9b56a4d529dd16d6453fcdab8",
            "tooltip": ""
          }
        },
        "e81c9f77f6a84ef5b4ab0da9a2fded2e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_cf673b65c5af42f9a001a406eb1d343c",
            "placeholder": "​",
            "style": "IPY_MODEL_7bfaa131eef443d3bb280ec04b6fcb93",
            "value": "\n<b>Pro Tip:</b> If you don't already have one, you can create a dedicated\n'notebooks' token with 'write' access, that you can then easily reuse for all\nnotebooks. </center>"
          }
        },
        "a3d32ab337aa4e9eb5cb57037d9f32c3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": "center",
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": "flex",
            "flex": null,
            "flex_flow": "column",
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "50%"
          }
        },
        "966b94f9bf1a4c229e071aa6d6dd749d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b4ea870eb86c4c6bb19908f148f6b21c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ac14c239ddd74f089cfb3fa09795523c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "aad28023b105476aa9316ed90b1efe3b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "eb91c481ba814dd7acf0c1ac2754ac25": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1bc3c8d3483543e190a59f1605d56a20": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4164ada8f2694f66b66a3e02d577842b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ad38f5d9b56a4d529dd16d6453fcdab8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "button_color": null,
            "font_weight": ""
          }
        },
        "cf673b65c5af42f9a001a406eb1d343c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7bfaa131eef443d3bb280ec04b6fcb93": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a7812bd6ee4940c89b9931a218f3ee0c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "LabelModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "LabelModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "LabelView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0f441aa96e0544b4a606699a6c2906e1",
            "placeholder": "​",
            "style": "IPY_MODEL_b10f55996000424398a9b5025de47188",
            "value": "Connecting..."
          }
        },
        "0f441aa96e0544b4a606699a6c2906e1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b10f55996000424398a9b5025de47188": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "659e4d43253043de849cdfe0ae916c01": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_652b42d1a64041219c3dc30b91e9082c",
              "IPY_MODEL_355f4fcabb6346cba9ad23c80108f726",
              "IPY_MODEL_d01f8b004be24bd4be3da9dcabefce85"
            ],
            "layout": "IPY_MODEL_6871c091b60442ac8c9b234ab5953fd6"
          }
        },
        "652b42d1a64041219c3dc30b91e9082c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4dbb5934431b46eba7ef658f1c3bea82",
            "placeholder": "​",
            "style": "IPY_MODEL_94d955e95a1d4cab9cad3ab4a9a1b862",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "355f4fcabb6346cba9ad23c80108f726": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_532e3146ca1d419f9c3572ca0d145e07",
            "max": 8,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_c860485238e04ca7a4e894464de12046",
            "value": 8
          }
        },
        "d01f8b004be24bd4be3da9dcabefce85": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6ff6221bee6b4177a08ebb69ac0fb7fe",
            "placeholder": "​",
            "style": "IPY_MODEL_597543cedda84942994cf61dc2f1dad1",
            "value": " 8/8 [00:00&lt;00:00, 17.90it/s]"
          }
        },
        "6871c091b60442ac8c9b234ab5953fd6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4dbb5934431b46eba7ef658f1c3bea82": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "94d955e95a1d4cab9cad3ab4a9a1b862": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "532e3146ca1d419f9c3572ca0d145e07": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c860485238e04ca7a4e894464de12046": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "6ff6221bee6b4177a08ebb69ac0fb7fe": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "597543cedda84942994cf61dc2f1dad1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/QuanNguyen28/Barefoot/blob/main/RAG_demo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pandas faiss-cpu sentence-transformers openai"
      ],
      "metadata": {
        "collapsed": true,
        "id": "aRtUtdd9kqbn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "726f3de5-8ebe-41d5-a2f9-81d3cd920b0e"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: faiss-cpu in /usr/local/lib/python3.11/dist-packages (1.11.0)\n",
            "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.11/dist-packages (3.4.1)\n",
            "Requirement already satisfied: openai in /usr/local/lib/python3.11/dist-packages (1.76.0)\n",
            "Requirement already satisfied: numpy>=1.23.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from faiss-cpu) (24.2)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (4.51.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (4.67.1)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (2.6.0+cu124)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (1.6.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (1.15.2)\n",
            "Requirement already satisfied: huggingface-hub>=0.20.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (0.30.2)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (11.2.1)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from openai) (4.9.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from openai) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from openai) (0.28.1)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from openai) (0.9.0)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.11/dist-packages (from openai) (2.11.3)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.11 in /usr/local/lib/python3.11/dist-packages (from openai) (4.13.2)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5,>=3.5.0->openai) (3.10)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->openai) (2025.1.31)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->openai) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.16.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (3.18.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2025.3.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (6.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2.32.3)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.1 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->openai) (2.33.1)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->openai) (0.4.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2024.11.6)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.5.3)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->sentence-transformers) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->sentence-transformers) (3.6.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.11.0->sentence-transformers) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.4.1)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2.4.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "19MH0oRjIIt2",
        "outputId": "34b32849-e6c7-4a2f-c246-ade007d37f7d"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Đọc và tiền xử lý dữ liệu tin tức từ các nguồn"
      ],
      "metadata": {
        "id": "AR9uffumn69R"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hWwOoFcjjPxW",
        "outputId": "312d5870-a234-4830-b2fa-28af8deb7f1b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "             record_date                 date ticker  \\\n",
            "0    2025-03-12 00:00:00  2025-03-12 00:00:00    FPT   \n",
            "1    2025-03-11 00:00:00  2025-03-11 00:00:00    FPT   \n",
            "2    2025-03-11 00:00:00  2025-03-11 00:00:00    FPT   \n",
            "3    2025-03-11 00:00:00  2025-03-11 00:00:00    FPT   \n",
            "4    2025-03-11 00:00:00  2025-03-11 00:00:00    FPT   \n",
            "..                   ...                  ...    ...   \n",
            "982  2023-05-22 00:00:00  2023-06-20 00:00:00    CMG   \n",
            "983  2023-04-26 00:00:00  2023-05-25 00:00:00    CMG   \n",
            "984  2023-03-23 00:00:00  2023-04-21 00:00:00    CMG   \n",
            "985  2023-03-15 00:00:00  2023-04-13 00:00:00    CMG   \n",
            "986  2023-03-14 00:00:00  2023-03-14 00:00:00    CMG   \n",
            "\n",
            "                                                  text    source  \n",
            "0    Phiên 12/3: Khối ngoại bán chiến biến hơn 900 ...     cafef  \n",
            "1    Chứng minh ngày mai (12-3): VN-Index tiếp tục ...     cafef  \n",
            "2    FPT \"Bắt tay\" Tỉnh Bắc Giang phát triển toàn d...     cafef  \n",
            "3    CTCK tự doanh không mong đợi trở lại \"gom\" một...     cafef  \n",
            "4    Chứng chỉ thoát hiểm 'phút 89'. Tóm tắt thị tr...     cafef  \n",
            "..                                                 ...       ...  \n",
            "982  Loại giao dịch: GD của người liên quan. Người ...  internal  \n",
            "983  Loại giao dịch: GD của người liên quan. Người ...  internal  \n",
            "984  Loại giao dịch: GD của người liên quan. Người ...  internal  \n",
            "985  Loại giao dịch: GD của người liên quan. Người ...  internal  \n",
            "986  Loại giao dịch: GD CĐ lớn. Người thực hiện: Py...  internal  \n",
            "\n",
            "[987 rows x 5 columns]\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import json\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import faiss\n",
        "import openai\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "def clean_text(text):\n",
        "    if pd.isna(text):\n",
        "        return \"\"\n",
        "    text = re.sub(r'<[^>]+>', '', str(text))\n",
        "    text = re.sub(r'<[^>]+>|[\\*\\#\\@]', '', text)\n",
        "    text = re.sub(r'\\s+', ' ', text)\n",
        "    return text.strip()\n",
        "\n",
        "def extract_ticker(text):\n",
        "    text = str(text).upper()\n",
        "    # Thêm các pattern phức tạp hơn và xử lý viết tắt\n",
        "    patterns = [\n",
        "        r'\\b(FPT|CMG)\\b',\n",
        "        r'\\b(FPT\\d*[A-Z]*)\\b',\n",
        "        r'\\b(CMG\\d*[A-Z]*)\\b'\n",
        "    ]\n",
        "    for pattern in patterns:\n",
        "        match = re.search(pattern, text)\n",
        "        if match:\n",
        "            return match.group(0)\n",
        "    return 'UNKNOWN'\n",
        "\n",
        "def preprocess_news(df, source_label):\n",
        "    title_col = 'title' if 'title' in df.columns else df.columns[0]\n",
        "    content_col = 'summary' if 'summary' in df.columns else df.columns[1]\n",
        "\n",
        "    # Cột ngày\n",
        "    if 'date' in df.columns:\n",
        "        date_col = 'date'\n",
        "    else:\n",
        "        possible_date = [col for col in df.columns if \"date\" in col.lower() or \"ngày\" in col.lower()]\n",
        "        if possible_date:\n",
        "            date_col = possible_date[0]\n",
        "        else:\n",
        "            raise ValueError(f\"Không tìm thấy cột ngày trong DataFrame {source_label}\")\n",
        "\n",
        "    # Làm sạch\n",
        "    df['title'] = df[title_col].apply(clean_text)\n",
        "    df['content'] = df[content_col].apply(clean_text)\n",
        "    df['text'] = df['title'] + \". \" + df['content']\n",
        "\n",
        "    # Parse ngày: mặc định mm/dd/yyyy → dayfirst=False\n",
        "    df['date'] = pd.to_datetime(df[date_col], errors='coerce', dayfirst=False)\n",
        "\n",
        "    df['source'] = source_label\n",
        "    df['record_date'] = df['date']\n",
        "\n",
        "    ticker_col = 'ticker' if 'ticker' in df.columns else None\n",
        "\n",
        "    # Nếu có ticker thì dùng, không thì trích\n",
        "    if ticker_col:\n",
        "        df['ticker'] = df[ticker_col]\n",
        "    else:\n",
        "        df['ticker'] = df['text'].apply(extract_ticker)\n",
        "\n",
        "    return df[['record_date', 'date', 'ticker', 'text', 'source']]\n",
        "\n",
        "def process_divided(df, source_label):\n",
        "\n",
        "    # Đầu tiên, chuẩn hóa tên cột về dạng dễ xử lý nếu cần\n",
        "    df.columns = df.columns.str.strip().str.lower()\n",
        "\n",
        "    # Đổi tên cho dễ code\n",
        "    rename_mapping = {\n",
        "        'exchange': 'exchange',\n",
        "        'ex-dividend date': 'ex_dividend_date',\n",
        "        'record date': 'record_date',\n",
        "        'execution date': 'execution_date',\n",
        "        'event content': 'event_content',\n",
        "        'event type': 'event_type'\n",
        "    }\n",
        "    df = df.rename(columns=rename_mapping)\n",
        "\n",
        "    # Tạo cột mới gộp thông tin\n",
        "    def combine_event_info(row):\n",
        "        parts = []\n",
        "        parts.append(f\"Sàn giao dịch: {row['exchange'] if pd.notna(row['exchange']) and row['exchange'] else 'UNKNOWN'}.\")\n",
        "        parts.append(f\"Ngày giao dịch không hưởng quyền: {row['ex_dividend_date'] if pd.notna(row['ex_dividend_date']) and row['ex_dividend_date'] else 'UNKNOWN'}.\")\n",
        "        parts.append(f\"Ngày chốt danh sách: {row['record_date'] if pd.notna(row['record_date']) and row['record_date'] else 'UNKNOWN'}.\")\n",
        "        parts.append(f\"Ngày thực hiện: {row['execution_date'] if pd.notna(row['execution_date']) and row['execution_date'] else 'UNKNOWN'}.\")\n",
        "        parts.append(f\"Nội dung sự kiện: {row['event_content'] if pd.notna(row['event_content']) and row['event_content'] else 'UNKNOWN'}.\")\n",
        "        parts.append(f\"Loại sự kiện: {row['event_type'] if pd.notna(row['event_type']) and row['event_type'] else 'UNKNOWN'}.\")\n",
        "        return \" \".join(parts)\n",
        "\n",
        "    df['text'] = df.apply(combine_event_info, axis=1)\n",
        "    df['date'] = pd.to_datetime(df['execution_date'], errors='coerce', dayfirst=True)\n",
        "    df['record_date'] = pd.to_datetime(df['record_date'], errors='coerce', dayfirst=True)\n",
        "\n",
        "    df['source'] = source_label\n",
        "    df['ticker'] = df['stockid'] if 'stockid' in df.columns else 'UNKNOWN'\n",
        "    # df[['record_date', 'date', 'ticker', 'text', 'source']]\n",
        "\n",
        "    return df[['record_date', 'date', 'ticker', 'text', 'source']]\n",
        "\n",
        "def process_shareholder(df, source_label):\n",
        "\n",
        "    # Đầu tiên, chuẩn hóa tên cột về dạng dễ xử lý nếu cần\n",
        "    df.columns = df.columns.str.strip().str.lower()\n",
        "\n",
        "    # Đổi tên cho dễ code\n",
        "    rename_mapping = {\n",
        "        'exchange': 'exchange',\n",
        "        'ex-rights date': 'ex_rights_date',\n",
        "        'record date': 'record_date',\n",
        "        'execution date': 'execution_date',\n",
        "        'event type': 'event_type'\n",
        "    }\n",
        "    df = df.rename(columns=rename_mapping)\n",
        "\n",
        "    # Tạo cột mới gộp thông tin\n",
        "    def combine_event_info(row):\n",
        "        parts = []\n",
        "        parts.append(f\"Sàn giao dịch: {row['exchange'] if pd.notna(row['exchange']) and row['exchange'] else 'UNKNOWN'}.\")\n",
        "        parts.append(f\"Ngày giao dịch không hưởng quyền: {row['ex_rights_date'] if pd.notna(row['ex_rights_date']) and row['ex_rights_date'] else 'UNKNOWN'}.\")\n",
        "        parts.append(f\"Ngày chốt danh sách: {row['record_date'] if pd.notna(row['record_date']) and row['record_date'] else 'UNKNOWN'}.\")\n",
        "        parts.append(f\"Ngày thực hiện: {row['execution_date'] if pd.notna(row['execution_date']) and row['execution_date'] else 'UNKNOWN'}.\")\n",
        "        parts.append(f\"Loại sự kiện: {row['event_type'] if pd.notna(row['event_type']) and row['event_type'] else 'UNKNOWN'}.\")\n",
        "        return \" \".join(parts)\n",
        "\n",
        "\n",
        "    df['text'] = df.apply(combine_event_info, axis=1)\n",
        "    df['date'] = pd.to_datetime(df['execution_date'], errors='coerce', dayfirst=True)\n",
        "    df['record_date'] = pd.to_datetime(df['record_date'], errors='coerce', dayfirst=True)\n",
        "    df['source'] = source_label\n",
        "    df['ticker'] = df['stockid'] if 'stockid' in df.columns else 'UNKNOWN'\n",
        "    # df[['record_date', 'date', 'ticker', 'text', 'source']]\n",
        "\n",
        "    return df[['record_date', 'date', 'ticker', 'text', 'source']]\n",
        "\n",
        "\n",
        "def process_internal(df, source_label):\n",
        "    # Chuẩn hóa tên cột\n",
        "    df.columns = df.columns.str.strip().str.lower()\n",
        "\n",
        "    # Các cột bạn muốn gộp\n",
        "    columns_to_combine = [\n",
        "        'transaction type', 'executor name', 'executor position', 'related person name',\n",
        "        'related person position', 'relation', 'before transaction volume', 'before transaction percentage',\n",
        "        'registered transaction volume', 'registered from date', 'registered to date',\n",
        "        'executed transaction volume', 'executed from date', 'executed to date',\n",
        "        'after transaction volume', 'after transaction percentage'\n",
        "    ]\n",
        "\n",
        "    for col in columns_to_combine:\n",
        "        if col in df.columns:\n",
        "            df[col] = df[col].apply(clean_text)\n",
        "\n",
        "    # Hàm gộp thành text\n",
        "    def combine_fields(row):\n",
        "        parts = []\n",
        "        parts.append(f\"Loại giao dịch: {row['transaction type'] if pd.notna(row['transaction type']) and row['transaction type'] else 'UNKNOWN'}.\")\n",
        "        parts.append(f\"Người thực hiện: {row['executor name'] if pd.notna(row['executor name']) and row['executor name'] else 'UNKNOWN'}.\")\n",
        "        parts.append(f\"Chức vụ người thực hiện: {row['executor position'] if pd.notna(row['executor position']) and row['executor position'] else 'UNKNOWN'}.\")\n",
        "        parts.append(f\"Người liên quan: {row['related person name'] if pd.notna(row['related person name']) and row['related person name'] else 'UNKNOWN'}.\")\n",
        "        parts.append(f\"Chức vụ người liên quan: {row['related person position'] if pd.notna(row['related person position']) and row['related person position'] else 'UNKNOWN'}.\")\n",
        "        parts.append(f\"Quan hệ: {row['relation'] if pd.notna(row['relation']) and row['relation'] else 'UNKNOWN'}.\")\n",
        "        parts.append(f\"Số lượng trước giao dịch: {row['before transaction volume'] if pd.notna(row['before transaction volume']) and row['before transaction volume'] else 'UNKNOWN'}.\")\n",
        "        parts.append(f\"Tỷ lệ trước giao dịch: {row['before transaction percentage'] if pd.notna(row['before transaction percentage']) and row['before transaction percentage'] else 'UNKNOWN'}%.\")\n",
        "        parts.append(f\"Số lượng đăng ký: {row['registered transaction volume'] if pd.notna(row['registered transaction volume']) and row['registered transaction volume'] else 'UNKNOWN'}.\")\n",
        "        parts.append(f\"Ngày bắt đầu đăng ký: {row['registered from date'] if pd.notna(row['registered from date']) and row['registered from date'] else 'UNKNOWN'}.\")\n",
        "        parts.append(f\"Ngày kết thúc đăng ký: {row['registered to date'] if pd.notna(row['registered to date']) and row['registered to date'] else 'UNKNOWN'}.\")\n",
        "        parts.append(f\"Số lượng thực tế giao dịch: {row['executed transaction volume'] if pd.notna(row['executed transaction volume']) and row['executed transaction volume'] else 'UNKNOWN'}.\")\n",
        "        parts.append(f\"Ngày bắt đầu thực hiện: {row['executed from date'] if pd.notna(row['executed from date']) and row['executed from date'] else 'UNKNOWN'}.\")\n",
        "        parts.append(f\"Ngày kết thúc thực hiện: {row['executed to date'] if pd.notna(row['executed to date']) and row['executed to date'] else 'UNKNOWN'}.\")\n",
        "        parts.append(f\"Số lượng sau giao dịch: {row['after transaction volume'] if pd.notna(row['after transaction volume']) and row['after transaction volume'] else 'UNKNOWN'}.\")\n",
        "        parts.append(f\"Tỷ lệ sau giao dịch: {row['after transaction percentage'] if pd.notna(row['after transaction percentage']) and row['after transaction percentage'] else 'UNKNOWN'}%.\")\n",
        "        return \" \".join(parts)\n",
        "\n",
        "\n",
        "    # Tạo cột text\n",
        "    df['text'] = df.apply(combine_fields, axis=1)\n",
        "    df['date'] = pd.to_datetime(df['executed to date'], errors='coerce', dayfirst=True)\n",
        "    df['record_date'] = pd.to_datetime(df['executed from date'], errors='coerce', dayfirst=True)\n",
        "    df['source'] = source_label\n",
        "    df['ticker'] = df['stockid'] if 'stockid' in df.columns else 'UNKNOWN'\n",
        "\n",
        "    return df[['record_date', 'date', 'ticker', 'text', 'source']]\n",
        "\n",
        "\n",
        "\n",
        "#Đọc file từ các nguồn\n",
        "df_cafef = pd.read_excel(\"/content/drive/MyDrive/Barefoots/Vòng 2/CSV/DATA EXPLORER CONTEST/News - FPT & CMG/Data processed/CafeF_News_FPT_CMG.xlsx\")\n",
        "df_dividend = pd.read_csv(\"/content/drive/MyDrive/Barefoots/Vòng 2/CSV/DATA EXPLORER CONTEST/News - FPT & CMG/Data processed/3.2 (live & his) news_dividend_issue (FPT_CMG)_processed.csv\")\n",
        "df_shareholder = pd.read_csv(\"/content/drive/MyDrive/Barefoots/Vòng 2/CSV/DATA EXPLORER CONTEST/News - FPT & CMG/3.3 (live & his) news_shareholder_meeting (FPT_CMG)_processed.csv\")\n",
        "df_internal = pd.read_csv(\"/content/drive/MyDrive/Barefoots/Vòng 2/CSV/DATA EXPLORER CONTEST/News - FPT & CMG/3.4 (live & his) news_internal_transactions (FPT_CMG)_processed.csv\")\n",
        "\n",
        "#Tiền xử lý từng DataFrame\n",
        "df_cafef_clean = preprocess_news(df_cafef, \"cafef\").fillna(\"UNKNOWN\")\n",
        "df_dividend_clean = process_divided(df_dividend, \"dividend\").fillna(\"UNKNOWN\")\n",
        "df_shareholder_clean = process_shareholder(df_shareholder, \"shareholder\").fillna(\"UNKNOWN\")\n",
        "df_internal_clean = process_internal(df_internal, \"internal\").fillna(\"UNKNOWN\")\n",
        "\n",
        "# Gộp\n",
        "df_all_news = pd.concat([\n",
        "    df_cafef_clean, df_dividend_clean, df_shareholder_clean, df_internal_clean\n",
        "], ignore_index=True)\n",
        "\n",
        "\n",
        "#Hợp nhất tất cả dữ liệu tin tức\n",
        "df_all_news = pd.concat([\n",
        "    df_cafef_clean, df_dividend_clean, df_shareholder_clean, df_internal_clean\n",
        "], ignore_index=True)\n",
        "\n",
        "print(df_all_news)\n",
        "# save\n",
        "df_all_news.to_csv(\"/content/drive/MyDrive/Barefoots/Vòng 2/CSV/DATA EXPLORER CONTEST/News - FPT & CMG/Data processed/all_news.csv\", index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Chunking"
      ],
      "metadata": {
        "id": "iTnPCzp9oBWM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "import pandas as pd\n",
        "\n",
        "# Thiết lập bộ chunking\n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size=500,         # Số ký tự mục tiêu mỗi đoạn (~300-500 tokens)\n",
        "    chunk_overlap=50,       # Phần overlap giữa các chunk\n",
        "    separators=[\"\\n\\n\", \"\\n\", \".\", \" \", \"\"],  # Ưu tiên tách theo đoạn > câu > từ\n",
        ")\n",
        "\n",
        "# Áp dụng chunking vào từng dòng trong df_all_news\n",
        "chunks = []\n",
        "\n",
        "for idx, row in df_all_news.iterrows():\n",
        "    split_texts = text_splitter.split_text(row['text'])\n",
        "    for chunk_text in split_texts:\n",
        "        chunks.append({\n",
        "            \"text\": chunk_text,\n",
        "            \"ticker\": row['ticker'],\n",
        "            \"record_date\": row['record_date'],\n",
        "            \"date\": row['date'],\n",
        "            \"source\": row['source']\n",
        "        })\n",
        "\n",
        "df_chunks = pd.DataFrame(chunks)\n",
        "\n",
        "# Kết quả\n",
        "print(f\"Số lượng chunk tạo ra: {len(df_chunks)}\")\n",
        "print(df_chunks.head())\n",
        "df_chunks.to_csv(\"/content/drive/MyDrive/Barefoots/Vòng 2/CSV/DATA EXPLORER CONTEST/News - FPT & CMG/Data processed/chunks.csv\", index=False)\n"
      ],
      "metadata": {
        "id": "rfZQpuyqjRsB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e12ad1ea-0998-45eb-b7ba-1ec5c03309a8"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Số lượng chunk tạo ra: 4265\n",
            "                                                text ticker  \\\n",
            "0  Phiên 12/3: Khối ngoại bán chiến biến hơn 900 ...    FPT   \n",
            "1  . Hoạt động giao dịch của khối ngoại: Khối ngo...    FPT   \n",
            "2  . Tổng quan HNX và UPCOM: Trên HNX, khối ngoại...    FPT   \n",
            "3  . Nhìn chung, trong khi VN-Index cho thấy khả ...    FPT   \n",
            "4  Chứng minh ngày mai (12-3): VN-Index tiếp tục ...    FPT   \n",
            "\n",
            "           record_date                 date source  \n",
            "0  2025-03-12 00:00:00  2025-03-12 00:00:00  cafef  \n",
            "1  2025-03-12 00:00:00  2025-03-12 00:00:00  cafef  \n",
            "2  2025-03-12 00:00:00  2025-03-12 00:00:00  cafef  \n",
            "3  2025-03-12 00:00:00  2025-03-12 00:00:00  cafef  \n",
            "4  2025-03-11 00:00:00  2025-03-11 00:00:00  cafef  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#embedding"
      ],
      "metadata": {
        "id": "wa5ahH5poSgq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# embedding_model = SentenceTransformer(\"paraphrase-multilingual-MiniLM-L12-v2\")\n",
        "# embeddings = embedding_model.encode(df_chunks['text'].tolist(), show_progress_bar=True)\n",
        "# print(\"Embedding shape:\", embeddings.shape)\n"
      ],
      "metadata": {
        "id": "cAuiC2gwjWd1"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# login vào huggingface\n",
        "from huggingface_hub import login\n",
        "login()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17,
          "referenced_widgets": [
            "acef41c6d27e4b5f95fb9a1f0509f0aa",
            "c83a5c940b2f4c1a9250c30d980fc1da",
            "c2547ac8c5d64d73b3f34a2dd67cc3ed",
            "683170a03b4f45c9918da174b92f005f",
            "e2a854200d7042298702b0141271256d",
            "e81c9f77f6a84ef5b4ab0da9a2fded2e",
            "a3d32ab337aa4e9eb5cb57037d9f32c3",
            "966b94f9bf1a4c229e071aa6d6dd749d",
            "b4ea870eb86c4c6bb19908f148f6b21c",
            "ac14c239ddd74f089cfb3fa09795523c",
            "aad28023b105476aa9316ed90b1efe3b",
            "eb91c481ba814dd7acf0c1ac2754ac25",
            "1bc3c8d3483543e190a59f1605d56a20",
            "4164ada8f2694f66b66a3e02d577842b",
            "ad38f5d9b56a4d529dd16d6453fcdab8",
            "cf673b65c5af42f9a001a406eb1d343c",
            "7bfaa131eef443d3bb280ec04b6fcb93",
            "a7812bd6ee4940c89b9931a218f3ee0c",
            "0f441aa96e0544b4a606699a6c2906e1",
            "b10f55996000424398a9b5025de47188"
          ]
        },
        "id": "ln326QHybokn",
        "outputId": "95125cbb-6cc9-4f02-e645-deb0ec11ad0d"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "acef41c6d27e4b5f95fb9a1f0509f0aa"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModel\n",
        "import torch\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Tải mô hình PhoBERT\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"vinai/phobert-base\")\n",
        "model = AutoModel.from_pretrained(\"vinai/phobert-base\")\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model = model.to(device)\n",
        "\n",
        "# Hàm mean pooling\n",
        "def mean_pooling(model_output, attention_mask):\n",
        "    token_embeddings = model_output[0]  # (batch_size, seq_len, hidden_size)\n",
        "    input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
        "    sum_embeddings = torch.sum(token_embeddings * input_mask_expanded, 1)\n",
        "    sum_mask = torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n",
        "    return sum_embeddings / sum_mask\n",
        "\n",
        "# Hàm encode toàn bộ df_chunks['text']\n",
        "def encode_phobert(texts):\n",
        "    embeddings = []\n",
        "    for text in tqdm(texts, desc=\"Encoding with PhoBERT\"):\n",
        "        encoded_input = tokenizer(text, padding=True, truncation=True, return_tensors='pt', max_length=512)\n",
        "        # Move encoded_input to the same device as the model\n",
        "        encoded_input = encoded_input.to(device) # This line has been added to move the input to the GPU\n",
        "        with torch.no_grad():\n",
        "            model_output = model(**encoded_input)\n",
        "        sentence_embedding = mean_pooling(model_output, encoded_input['attention_mask'])\n",
        "        embeddings.append(sentence_embedding.squeeze(0).cpu().numpy())\n",
        "    return np.vstack(embeddings)\n",
        "\n",
        "# Dùng để embedding\n",
        "texts = df_chunks['text'].tolist()\n",
        "embeddings = encode_phobert(texts)\n",
        "\n",
        "print(\"Embedding shape:\", embeddings.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QxSKkwS9Hydd",
        "outputId": "84df3981-fe0a-4e13-9062-96ca2e625a70"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Encoding with PhoBERT: 100%|██████████| 4265/4265 [00:47<00:00, 90.24it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Embedding shape: (4265, 768)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Lưu trữ vào FAISS"
      ],
      "metadata": {
        "id": "TfHMmJSvoX4L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import faiss\n",
        "import numpy as np\n",
        "\n",
        "# Chuẩn bị dimension\n",
        "dimension = embeddings.shape[1]\n",
        "\n",
        "# Khởi tạo FAISS Index\n",
        "index = faiss.IndexFlatIP(dimension)   # Dùng Inner Product thay vì L2 (vì đã normalize rồi)\n",
        "\n",
        "# Normalize embedding trước khi add\n",
        "faiss.normalize_L2(embeddings)\n",
        "\n",
        "# Add vào FAISS index\n",
        "index.add(embeddings)\n",
        "\n",
        "print(\"FAISS index có số vector:\", index.ntotal)\n",
        "\n",
        "# Gán mapping index vào df_chunks\n",
        "df_chunks['embedding_index'] = range(len(df_chunks))\n"
      ],
      "metadata": {
        "id": "_i1PtrfwjWmR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0d393c9f-8c2c-4df6-e6aa-99e05d76429a"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "FAISS index có số vector: 4265\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Query Transformation"
      ],
      "metadata": {
        "id": "QoLksub1odNa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def retrieve(query, top_k=5):\n",
        "    \"\"\"\n",
        "    Nhận truy vấn dạng chuỗi,\n",
        "    tạo vector embedding cho truy vấn bằng PhoBERT,\n",
        "    normalize query vector,\n",
        "    tìm kiếm top_k vector (chunk) gần nhất từ FAISS index,\n",
        "    trả về danh sách văn bản.\n",
        "    \"\"\"\n",
        "    # Tokenize và encode query bằng PhoBERT\n",
        "    encoded_input = tokenizer(query, padding=True, truncation=True, return_tensors='pt', max_length=512)\n",
        "\n",
        "    # Move encoded_input to the same device as the model\n",
        "    encoded_input = encoded_input.to(device) # Move encoded_input to GPU if available\n",
        "\n",
        "    with torch.no_grad():\n",
        "        model_output = model(**encoded_input)\n",
        "\n",
        "    # Mean pooling\n",
        "    query_embedding = mean_pooling(model_output, encoded_input['attention_mask']).cpu().numpy()\n",
        "\n",
        "    # Normalize query vector\n",
        "    faiss.normalize_L2(query_embedding)\n",
        "\n",
        "    # Search trong FAISS\n",
        "    D, I = index.search(query_embedding, top_k)\n",
        "\n",
        "    # Lấy text chunks tương ứng\n",
        "    results = df_chunks.iloc[I[0]]['text'].tolist()\n",
        "    return results"
      ],
      "metadata": {
        "id": "GCUqrn1njZ8g"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: lấy kết quả từ truy vấn retrieve và chuyển chúng thành một chuỗi văn bản có thể hiển thị\n",
        "\n",
        "def retrieve_and_format(query, top_k=10):\n",
        "    results = retrieve(query, top_k)\n",
        "    formatted_results = \"\"\n",
        "    for i, result in enumerate(results):\n",
        "        formatted_results += f\"Thông tin {i+1}:\\n{result}\\n\"\n",
        "    return formatted_results"
      ],
      "metadata": {
        "id": "7Q-b9CsFKyoU"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"FPT có điểm gì nổi bật năm 2025?\"\n",
        "source_information = retrieve_and_format(query)\n",
        "print(\"Phản hồi từ hệ thống RAG:\")\n",
        "print(source_information)"
      ],
      "metadata": {
        "id": "5V8v6c04nV-5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f18b1971-2e94-40e2-a5b1-6fff225c50e7"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Phản hồi từ hệ thống RAG:\n",
            "Thông tin 1:\n",
            ". Ngành cơ điện dự kiến phục hồi với mức tăng trưởng dự báo 21% trong giai đoạn 2025-2028. Gemadept Corporation (GMD): Dự báo sản lượng thông qua cảng tăng trưởng trên 40% vào năm 2024. Các diễn biến mới dự kiến sẽ thúc đẩy tăng trưởng trung và dài hạn sau năm 2026. Hòa Phát Group (HPG): Dự kiến lợi nhuận tăng trưởng 26% vào năm 2025, được hỗ trợ bởi giá thép trong nước ổn định và tiêu dùng được cải thiện\n",
            "Thông tin 2:\n",
            "Nhóm công ty chứng khoán 'lăng xê' thuộc ngành nào trong năm 2025?. Tóm tắt thị trường (28/01/2025) Dự báo VN-Index: KBSV Research dự báo VN-Index sẽ đạt 1.460 điểm vào cuối năm 2025, tương ứng với mức tăng 16,7% trong EPS trung bình của các công ty niêm yết trên HOSE, với tỷ lệ P/E mục tiêu là 14,6, thấp hơn mức trung bình 10 năm là 16,6\n",
            "Thông tin 3:\n",
            "Năm rực rỡ của cổ phiếu “họ” FPT, vẫn còn một cái tên “lạc lõng”. Tóm tắt thị trường - Ngày 15 tháng 02 năm 2024 Thị trường chứng khoán Việt Nam đối mặt với nhiều thách thức trong năm 2024, khi VN-Index dao động quanh mức 1.200 điểm trong bối cảnh diễn biến cổ phiếu đầy biến động. Tuy nhiên, cổ phiếu FPT và các công ty liên kết nổi lên như những điểm sáng đáng kể. 1. Tập đoàn FPT (FPT): - Cổ phiếu tăng vọt lên mức cao kỷ lục 104\n",
            "Thông tin 4:\n",
            "Tập đoàn tư nhân lớn nhất Việt Nam lập kỷ lục mới, giá trị vốn hóa gần 230.000 tỷ lệ. Bản tin thị trường chứng khoán Việt Nam - 23/01/2025 Thị trường chứng khoán Việt Nam cho thấy xu hướng tích cực khi bước vào cuối năm Rồng. Đáng chú ý, cổ phiếu của Tập đoàn FPT đã tăng hơn 3% lên mức cao mới là 154.300 đồng/cổ phiếu, đánh dấu lần đầu tiên vượt qua mức này trong năm 2025. Trước đó, FPT đã lập kỷ lục chưa từng có với 42 lần đạt đỉnh vào năm 2024\n",
            "Thông tin 5:\n",
            "FPT đạt cột cột mới sau khi \"bắt tay\" NVIDIA mở nhà máy AI, đạt mục tiêu lọt vào top công ty lớn ở Nhật Bản vào năm 2027. Tóm tắt thị trường - Ngày 5 tháng 2 năm 2025 Hiệu suất công ty: FPT Corporation (Mã chứng khoán: FPT) đã báo cáo một cột mốc quan trọng, đạt doanh thu hơn 500 triệu đô la từ thị trường xuất khẩu chính của mình là Nhật Bản. Thành tích này phản ánh mức tăng trưởng hàng năm là 32,2% vào năm 2024, đạt mức tăng 36,3% khi tính bằng Yên Nhật\n",
            "Thông tin 6:\n",
            "Tập đoàn công nghệ số 1 Việt Nam thêm một lần đạt danh hiệu vào ngày cuối năm 2024. Tóm tắt tin tức tài chính - Thị trường chứng khoán Việt Nam (31/12/2024) Ngày giao dịch cuối cùng của năm 2024, thị trường chứng khoán Việt Nam diễn biến không mấy khởi sắc, mặc dù cổ phiếu FPT có mức tăng đáng kể. Cổ phiếu FPT đạt mức cao kỷ lục 153\n",
            "Thông tin 7:\n",
            ". Viettel đã mở rộng đáng kể trong lĩnh vực viễn thông và hậu cần. Xu hướng ngành: Ngành viễn thông đang chuyển đổi từ 2G sang 5G, nâng cao tiềm năng thị trường. FPT đang sẵn sàng trở thành công ty dẫn đầu về AI, với doanh thu dự kiến từ nhà máy AI đạt 100 triệu đô la vào năm 2025. Viettel Global đặt mục tiêu xóa sổ các khoản lỗ tồn đọng vào năm 2025, với lợi nhuận trước thuế vượt 6 nghìn tỷ đồng trong chín tháng đầu năm 2024. Các chỉ số thị trường: VN-Index dao động trong khoảng 1.200-1\n",
            "Thông tin 8:\n",
            ". Dự báo tăng trưởng trong tương lai: Nhìn về phía trước, SSI Research dự báo lợi nhuận của FPT có thể đạt khoảng 13,7 nghìn tỷ đồng vào năm 2025, nhờ vào lĩnh vực CNTT quốc tế và những đổi mới từ Nhà máy AI của FPT. Mảng này dự kiến sẽ chứng kiến mức tăng trưởng doanh thu là 29% và mức tăng trưởng lợi nhuận là 31%. FPT cũng đang mở rộng dấu ấn của mình với một nhà máy dịch vụ AI tại Việt Nam và Nhật Bản, dự kiến sẽ thúc đẩy triển vọng tăng trưởng trong tương lai\n",
            "Thông tin 9:\n",
            ".395 tỷ đồng, tăng 21%. Nếu đạt được, đây sẽ là năm năm liên tiếp tăng trưởng trên 20%. Điểm nổi bật về hiệu suất: Ngành CNTT dự kiến sẽ chứng kiến sự tăng trưởng đáng kể trên toàn cầu, với FPT dự kiến doanh thu tăng: 20% tại Hoa Kỳ 27-30% tại Nhật Bản. 25-30% tại Châu Á - Thái Bình Dương và EU. Với lợi thế về chi phí 15% so với các đối thủ cạnh tranh toàn cầu, FPT đã sẵn sàng cho vị thế cạnh tranh mạnh mẽ trong chuyển đổi số\n",
            "Thông tin 10:\n",
            "Lần đầu tiên Fortune công bố Top DN lớn nhất Đông Nam Á, điểm mặt 13 DN Việt lọt Top 100: Petrolimex, VinGroup, Thế giới Di động…. Tóm tắt thị trường: Những điểm nổi bật trong Báo cáo Fortune SEA 500 - Ngày & giờ: Ngày 18 tháng 6 năm 2024, lúc 15:30 chiều - Sự kiện chính: Công bố bảng xếp hạng Fortune SEA 500 đầu tiên, nêu bật 500 công ty hàng đầu tại Đông Nam Á dựa trên doanh thu năm 2023\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def format_prompt(query, context):\n",
        "    return f\"\"\"<s>[INST]\n",
        "Bạn là chuyên gia phân tích tài chính. Hãy trả lời câu hỏi dựa trên thông tin sau:\n",
        "Câu hỏi: {query}\n",
        "\n",
        "Thông tin tham khảo:\n",
        "{context}\n",
        "\n",
        "Yêu cầu:\n",
        "- Trả lời ngắn gọn, tự nhiên (không dấu đầu dòng)\n",
        "- Luôn ghi rõ ngày, nguồn thông tin nếu có\n",
        "- Nếu không đủ thông tin để kết luận, hãy nói rõ\n",
        "[/INST]\n",
        "\"\"\"\n",
        "print(format_prompt(query, source_information))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H22wI8E8lEZO",
        "outputId": "4aa5b7f7-7c33-4433-fba2-15ecf68e9b4e"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<s>[INST]\n",
            "Bạn là chuyên gia phân tích tài chính. Hãy trả lời câu hỏi dựa trên thông tin sau:\n",
            "Câu hỏi: FPT có điểm gì nổi bật năm 2025?\n",
            "\n",
            "Thông tin tham khảo:\n",
            "Thông tin 1:\n",
            ". Ngành cơ điện dự kiến phục hồi với mức tăng trưởng dự báo 21% trong giai đoạn 2025-2028. Gemadept Corporation (GMD): Dự báo sản lượng thông qua cảng tăng trưởng trên 40% vào năm 2024. Các diễn biến mới dự kiến sẽ thúc đẩy tăng trưởng trung và dài hạn sau năm 2026. Hòa Phát Group (HPG): Dự kiến lợi nhuận tăng trưởng 26% vào năm 2025, được hỗ trợ bởi giá thép trong nước ổn định và tiêu dùng được cải thiện\n",
            "Thông tin 2:\n",
            "Nhóm công ty chứng khoán 'lăng xê' thuộc ngành nào trong năm 2025?. Tóm tắt thị trường (28/01/2025) Dự báo VN-Index: KBSV Research dự báo VN-Index sẽ đạt 1.460 điểm vào cuối năm 2025, tương ứng với mức tăng 16,7% trong EPS trung bình của các công ty niêm yết trên HOSE, với tỷ lệ P/E mục tiêu là 14,6, thấp hơn mức trung bình 10 năm là 16,6\n",
            "Thông tin 3:\n",
            "Năm rực rỡ của cổ phiếu “họ” FPT, vẫn còn một cái tên “lạc lõng”. Tóm tắt thị trường - Ngày 15 tháng 02 năm 2024 Thị trường chứng khoán Việt Nam đối mặt với nhiều thách thức trong năm 2024, khi VN-Index dao động quanh mức 1.200 điểm trong bối cảnh diễn biến cổ phiếu đầy biến động. Tuy nhiên, cổ phiếu FPT và các công ty liên kết nổi lên như những điểm sáng đáng kể. 1. Tập đoàn FPT (FPT): - Cổ phiếu tăng vọt lên mức cao kỷ lục 104\n",
            "Thông tin 4:\n",
            "Tập đoàn tư nhân lớn nhất Việt Nam lập kỷ lục mới, giá trị vốn hóa gần 230.000 tỷ lệ. Bản tin thị trường chứng khoán Việt Nam - 23/01/2025 Thị trường chứng khoán Việt Nam cho thấy xu hướng tích cực khi bước vào cuối năm Rồng. Đáng chú ý, cổ phiếu của Tập đoàn FPT đã tăng hơn 3% lên mức cao mới là 154.300 đồng/cổ phiếu, đánh dấu lần đầu tiên vượt qua mức này trong năm 2025. Trước đó, FPT đã lập kỷ lục chưa từng có với 42 lần đạt đỉnh vào năm 2024\n",
            "Thông tin 5:\n",
            "FPT đạt cột cột mới sau khi \"bắt tay\" NVIDIA mở nhà máy AI, đạt mục tiêu lọt vào top công ty lớn ở Nhật Bản vào năm 2027. Tóm tắt thị trường - Ngày 5 tháng 2 năm 2025 Hiệu suất công ty: FPT Corporation (Mã chứng khoán: FPT) đã báo cáo một cột mốc quan trọng, đạt doanh thu hơn 500 triệu đô la từ thị trường xuất khẩu chính của mình là Nhật Bản. Thành tích này phản ánh mức tăng trưởng hàng năm là 32,2% vào năm 2024, đạt mức tăng 36,3% khi tính bằng Yên Nhật\n",
            "Thông tin 6:\n",
            "Tập đoàn công nghệ số 1 Việt Nam thêm một lần đạt danh hiệu vào ngày cuối năm 2024. Tóm tắt tin tức tài chính - Thị trường chứng khoán Việt Nam (31/12/2024) Ngày giao dịch cuối cùng của năm 2024, thị trường chứng khoán Việt Nam diễn biến không mấy khởi sắc, mặc dù cổ phiếu FPT có mức tăng đáng kể. Cổ phiếu FPT đạt mức cao kỷ lục 153\n",
            "Thông tin 7:\n",
            ". Viettel đã mở rộng đáng kể trong lĩnh vực viễn thông và hậu cần. Xu hướng ngành: Ngành viễn thông đang chuyển đổi từ 2G sang 5G, nâng cao tiềm năng thị trường. FPT đang sẵn sàng trở thành công ty dẫn đầu về AI, với doanh thu dự kiến từ nhà máy AI đạt 100 triệu đô la vào năm 2025. Viettel Global đặt mục tiêu xóa sổ các khoản lỗ tồn đọng vào năm 2025, với lợi nhuận trước thuế vượt 6 nghìn tỷ đồng trong chín tháng đầu năm 2024. Các chỉ số thị trường: VN-Index dao động trong khoảng 1.200-1\n",
            "Thông tin 8:\n",
            ". Dự báo tăng trưởng trong tương lai: Nhìn về phía trước, SSI Research dự báo lợi nhuận của FPT có thể đạt khoảng 13,7 nghìn tỷ đồng vào năm 2025, nhờ vào lĩnh vực CNTT quốc tế và những đổi mới từ Nhà máy AI của FPT. Mảng này dự kiến sẽ chứng kiến mức tăng trưởng doanh thu là 29% và mức tăng trưởng lợi nhuận là 31%. FPT cũng đang mở rộng dấu ấn của mình với một nhà máy dịch vụ AI tại Việt Nam và Nhật Bản, dự kiến sẽ thúc đẩy triển vọng tăng trưởng trong tương lai\n",
            "Thông tin 9:\n",
            ".395 tỷ đồng, tăng 21%. Nếu đạt được, đây sẽ là năm năm liên tiếp tăng trưởng trên 20%. Điểm nổi bật về hiệu suất: Ngành CNTT dự kiến sẽ chứng kiến sự tăng trưởng đáng kể trên toàn cầu, với FPT dự kiến doanh thu tăng: 20% tại Hoa Kỳ 27-30% tại Nhật Bản. 25-30% tại Châu Á - Thái Bình Dương và EU. Với lợi thế về chi phí 15% so với các đối thủ cạnh tranh toàn cầu, FPT đã sẵn sàng cho vị thế cạnh tranh mạnh mẽ trong chuyển đổi số\n",
            "Thông tin 10:\n",
            "Lần đầu tiên Fortune công bố Top DN lớn nhất Đông Nam Á, điểm mặt 13 DN Việt lọt Top 100: Petrolimex, VinGroup, Thế giới Di động…. Tóm tắt thị trường: Những điểm nổi bật trong Báo cáo Fortune SEA 500 - Ngày & giờ: Ngày 18 tháng 6 năm 2024, lúc 15:30 chiều - Sự kiện chính: Công bố bảng xếp hạng Fortune SEA 500 đầu tiên, nêu bật 500 công ty hàng đầu tại Đông Nam Á dựa trên doanh thu năm 2023\n",
            "\n",
            "\n",
            "Yêu cầu:\n",
            "- Trả lời ngắn gọn, tự nhiên (không dấu đầu dòng)\n",
            "- Luôn ghi rõ ngày, nguồn thông tin nếu có\n",
            "- Nếu không đủ thông tin để kết luận, hãy nói rõ\n",
            "[/INST]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Triển khai"
      ],
      "metadata": {
        "id": "bVULZWC-fVGS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install transformers"
      ],
      "metadata": {
        "id": "kG7rllnv27-a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dc60ce2a-19dc-4e6f-cb0e-ef128faf2709"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.51.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.30.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (2025.3.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.13.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.1.31)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tiktoken"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KKfRyAjCK5zx",
        "outputId": "2d535186-5a21-41d0-bc94-80e4ee22f8cd"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tiktoken in /usr/local/lib/python3.11/dist-packages (0.9.0)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.11/dist-packages (from tiktoken) (2024.11.6)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.11/dist-packages (from tiktoken) (2.32.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken) (2025.1.31)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers_stream_generator"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UrPYHGmnLK_P",
        "outputId": "211c2ca8-0cb1-4e03-8c86-b65e102b6397"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers_stream_generator in /usr/local/lib/python3.11/dist-packages (0.0.5)\n",
            "Requirement already satisfied: transformers>=4.26.1 in /usr/local/lib/python3.11/dist-packages (from transformers_stream_generator) (4.51.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers>=4.26.1->transformers_stream_generator) (3.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.26.1->transformers_stream_generator) (0.30.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.26.1->transformers_stream_generator) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.26.1->transformers_stream_generator) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.26.1->transformers_stream_generator) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.26.1->transformers_stream_generator) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers>=4.26.1->transformers_stream_generator) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.26.1->transformers_stream_generator) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.26.1->transformers_stream_generator) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.26.1->transformers_stream_generator) (4.67.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers>=4.26.1->transformers_stream_generator) (2025.3.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers>=4.26.1->transformers_stream_generator) (4.13.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers>=4.26.1->transformers_stream_generator) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers>=4.26.1->transformers_stream_generator) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers>=4.26.1->transformers_stream_generator) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers>=4.26.1->transformers_stream_generator) (2025.1.31)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# # Load model directly\n",
        "# from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "\n",
        "# tokenizer = AutoTokenizer.from_pretrained(\"google/gemma-2b-it\")\n",
        "# model = AutoModelForCausalLM.from_pretrained(\"google/gemma-2b-it\")\n"
      ],
      "metadata": {
        "id": "fyqZOKV0f251"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_response_from_gemma(query, context_chunks, max_new_tokens=500):\n",
        "    prompt = format_prompt(query, context_chunks)\n",
        "\n",
        "    # Tokenize + đưa vào GPU\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=2048)\n",
        "    inputs = {k: v.to(model.device) for k, v in inputs.items()}\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model.generate(\n",
        "            **inputs,\n",
        "            max_new_tokens=max_new_tokens,\n",
        "            temperature=0.7,\n",
        "            do_sample=False,\n",
        "            eos_token_id=tokenizer.eos_token_id\n",
        "        )\n",
        "\n",
        "    # Decode kết quả\n",
        "    decoded = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "    # Tách phần trả lời sau <start_of_turn>model\n",
        "    if \"<start_of_turn>model\" in decoded:\n",
        "        return decoded.split(\"<start_of_turn>model\")[-1].strip()\n",
        "    return decoded.strip()\n"
      ],
      "metadata": {
        "id": "ND-XL0cTJ6fb"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "import torch\n",
        "\n",
        "model_name = \"Qwen/Qwen-7B-Chat\"\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    torch_dtype=torch.float16,\n",
        "    #device_map=\"auto\", # Comment out device_map=\"auto\"\n",
        "    trust_remote_code=True,\n",
        "    # Added: offload_folder for disk offloading\n",
        "    offload_folder=\"offload\", # Specify a folder to offload to\n",
        "    # Added: low_cpu_mem_usage for optimized CPU RAM usage\n",
        "    low_cpu_mem_usage=True\n",
        ")\n",
        "\n",
        "# Explicitly move the model to the GPU if available\n",
        "if torch.cuda.is_available():\n",
        "    model.cuda() # or model.to('cuda')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 156,
          "referenced_widgets": [
            "659e4d43253043de849cdfe0ae916c01",
            "652b42d1a64041219c3dc30b91e9082c",
            "355f4fcabb6346cba9ad23c80108f726",
            "d01f8b004be24bd4be3da9dcabefce85",
            "6871c091b60442ac8c9b234ab5953fd6",
            "4dbb5934431b46eba7ef658f1c3bea82",
            "94d955e95a1d4cab9cad3ab4a9a1b862",
            "532e3146ca1d419f9c3572ca0d145e07",
            "c860485238e04ca7a4e894464de12046",
            "6ff6221bee6b4177a08ebb69ac0fb7fe",
            "597543cedda84942994cf61dc2f1dad1"
          ]
        },
        "id": "aS69U8IWBwq4",
        "outputId": "f83c845b-8049-4a25-c13d-7b2d7ab71abf"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:transformers_modules.Qwen.Qwen-7B-Chat.93a65d34827a3cc269b727e67004743b723e2f83.modeling_qwen:The model is automatically converting to bf16 for faster inference. If you want to disable the automatic precision, please manually add bf16/fp16/fp32=True to \"AutoModelForCausalLM.from_pretrained\".\n",
            "WARNING:transformers_modules.Qwen.Qwen-7B-Chat.93a65d34827a3cc269b727e67004743b723e2f83.modeling_qwen:Try importing flash-attention for faster inference...\n",
            "WARNING:transformers_modules.Qwen.Qwen-7B-Chat.93a65d34827a3cc269b727e67004743b723e2f83.modeling_qwen:Warning: import flash_attn rotary fail, please install FlashAttention rotary to get higher efficiency https://github.com/Dao-AILab/flash-attention/tree/main/csrc/rotary\n",
            "WARNING:transformers_modules.Qwen.Qwen-7B-Chat.93a65d34827a3cc269b727e67004743b723e2f83.modeling_qwen:Warning: import flash_attn rms_norm fail, please install FlashAttention layer_norm to get higher efficiency https://github.com/Dao-AILab/flash-attention/tree/main/csrc/layer_norm\n",
            "WARNING:transformers_modules.Qwen.Qwen-7B-Chat.93a65d34827a3cc269b727e67004743b723e2f83.modeling_qwen:Warning: import flash_attn fail, please install FlashAttention to get higher efficiency https://github.com/Dao-AILab/flash-attention\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/8 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "659e4d43253043de849cdfe0ae916c01"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_response_from_mistral(query, context_chunks, max_new_tokens=512):\n",
        "    prompt = format_prompt(query, context_chunks)\n",
        "\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=2048)\n",
        "    # Change: Move the entire model to the desired device\n",
        "    if torch.cuda.is_available():\n",
        "        model.to('cuda')\n",
        "    inputs = {k: v.to(model.device) for k, v in inputs.items()}\n",
        "\n",
        "    with torch.no_grad():\n",
        "        # Removed attention_mask from the separate argument\n",
        "        outputs = model.generate(\n",
        "            **inputs,\n",
        "            max_new_tokens=max_new_tokens,\n",
        "            temperature=0.7,\n",
        "            do_sample=False,\n",
        "            eos_token_id=tokenizer.eos_token_id,\n",
        "            # attention_mask=inputs.get(\"attention_mask\", None) # removed this line\n",
        "        )\n",
        "\n",
        "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    # Loại bỏ phần prompt nếu cần\n",
        "    if \"[/INST]\" in response:\n",
        "        return response.split(\"[/INST]\")[-1].strip()\n",
        "    return response.strip()"
      ],
      "metadata": {
        "id": "LEiTGEewBurx"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "context_chunks = format_prompt(query, source_information)\n",
        "answer = generate_response_from_mistral(query, context_chunks, 512)\n",
        "print(\"🧠 Câu trả lời từ Gemma:\")\n",
        "print(answer)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 567
        },
        "id": "yxRU0aFN3Hwx",
        "outputId": "636a6895-339a-4a8e-d1bb-47a6917246c2"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.7` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.8` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:653: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "OutOfMemoryError",
          "evalue": "CUDA out of memory. Tried to allocate 26.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 16.12 MiB is free. Process 42336 has 14.72 GiB memory in use. Of the allocated memory 14.57 GiB is allocated by PyTorch, and 30.45 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-40-d358db14877e>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mcontext_chunks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mformat_prompt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msource_information\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerate_response_from_mistral\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontext_chunks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m512\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"🧠 Câu trả lời từ Gemma:\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-39-78976a08fa5a>\u001b[0m in \u001b[0;36mgenerate_response_from_mistral\u001b[0;34m(query, context_chunks, max_new_tokens)\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0;31m# Removed attention_mask from the separate argument\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m         outputs = model.generate(\n\u001b[0m\u001b[1;32m     13\u001b[0m             \u001b[0;34m**\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m             \u001b[0mmax_new_tokens\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_new_tokens\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/.cache/huggingface/modules/transformers_modules/Qwen/Qwen-7B-Chat/93a65d34827a3cc269b727e67004743b723e2f83/modeling_qwen.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, **kwargs)\u001b[0m\n\u001b[1;32m   1257\u001b[0m                 \u001b[0mlogits_processor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstop_words_logits_processor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1259\u001b[0;31m         return super().generate(\n\u001b[0m\u001b[1;32m   1260\u001b[0m             \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1261\u001b[0m             \u001b[0mgeneration_config\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgeneration_config\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/_contextlib.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    114\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mctx_factory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/generation/utils.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, use_model_defaults, **kwargs)\u001b[0m\n\u001b[1;32m   2463\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2464\u001b[0m             \u001b[0;31m# 12. run sample (it degenerates to greedy search when `generation_config.do_sample=False`)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2465\u001b[0;31m             result = self._sample(\n\u001b[0m\u001b[1;32m   2466\u001b[0m                 \u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2467\u001b[0m                 \u001b[0mlogits_processor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprepared_logits_processor\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/generation/utils.py\u001b[0m in \u001b[0;36m_sample\u001b[0;34m(self, input_ids, logits_processor, stopping_criteria, generation_config, synced_gpus, streamer, **model_kwargs)\u001b[0m\n\u001b[1;32m   3429\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3430\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_prefill\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3431\u001b[0;31m                 \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mmodel_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3432\u001b[0m                 \u001b[0mis_prefill\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3433\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/.cache/huggingface/modules/transformers_modules/Qwen/Qwen-7B-Chat/93a65d34827a3cc269b727e67004743b723e2f83/modeling_qwen.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, past_key_values, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1041\u001b[0m         )\n\u001b[1;32m   1042\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1043\u001b[0;31m         transformer_outputs = self.transformer(\n\u001b[0m\u001b[1;32m   1044\u001b[0m             \u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1045\u001b[0m             \u001b[0mpast_key_values\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpast_key_values\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/.cache/huggingface/modules/transformers_modules/Qwen/Qwen-7B-Chat/93a65d34827a3cc269b727e67004743b723e2f83/modeling_qwen.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, past_key_values, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    889\u001b[0m                 )\n\u001b[1;32m    890\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 891\u001b[0;31m                 outputs = block(\n\u001b[0m\u001b[1;32m    892\u001b[0m                     \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    893\u001b[0m                     \u001b[0mlayer_past\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlayer_past\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/.cache/huggingface/modules/transformers_modules/Qwen/Qwen-7B-Chat/93a65d34827a3cc269b727e67004743b723e2f83/modeling_qwen.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, rotary_pos_emb_list, layer_past, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, use_cache, output_attentions)\u001b[0m\n\u001b[1;32m    608\u001b[0m         \u001b[0mlayernorm_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mln_1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    609\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 610\u001b[0;31m         attn_outputs = self.attn(\n\u001b[0m\u001b[1;32m    611\u001b[0m             \u001b[0mlayernorm_output\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    612\u001b[0m             \u001b[0mrotary_pos_emb_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/.cache/huggingface/modules/transformers_modules/Qwen/Qwen-7B-Chat/93a65d34827a3cc269b727e67004743b723e2f83/modeling_qwen.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, rotary_pos_emb_list, layer_past, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, output_attentions, use_cache)\u001b[0m\n\u001b[1;32m    526\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    527\u001b[0m                     \u001b[0mattention_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcausal_mask\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 528\u001b[0;31m                 attn_output = F.scaled_dot_product_attention(\n\u001b[0m\u001b[1;32m    529\u001b[0m                     \u001b[0mquery\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattn_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    530\u001b[0m                 ).transpose(1, 2)\n",
            "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 26.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 16.12 MiB is free. Process 42336 has 14.72 GiB memory in use. Of the allocated memory 14.57 GiB is allocated by PyTorch, and 30.45 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
          ]
        }
      ]
    }
  ]
}